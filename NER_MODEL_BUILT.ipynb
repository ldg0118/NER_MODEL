{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_MODEL_BUILT.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNvdnDTlU9Rj0N3JGl9CjLH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldg0118/NER_MODEL/blob/main/NER_MODEL_BUILT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER model built based on BERT"
      ],
      "metadata": {
        "id": "GJ60WxfJwaay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset used: conll dataset \n",
        "*   Downloaded from https://deepai.org/dataset/conll-2003-english\n",
        "*   Detailed explanation https://github.com/huggingface/datasets/tree/master/datasets/conll2003\n",
        "\n",
        "Code reference:\n",
        "https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=VuUdX_fImswO\n",
        "\n",
        "Model used: BertForTokenClassification\n",
        "https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForTokenClassification\n",
        "\n",
        "Model Base:\n",
        "https://github.com/chambliss/Multilingual_NER/blob/master/python/utils/main_utils.py#L118\n",
        "\n",
        "GitHub for the detailed model built process: https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L512\n",
        "\n",
        "Loss function: CrossEntropyLoss\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n"
      ],
      "metadata": {
        "id": "FYJI-w-Xxy3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import packages"
      ],
      "metadata": {
        "id": "8UwEgsjovum7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JMyoJFur7aol"
      },
      "outputs": [],
      "source": [
        "#!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "d8PGgxVW7zO1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers seqeval[gpu]"
      ],
      "metadata": {
        "id": "RiAUxGm_R-5T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import cuda\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "from seqeval.metrics import classification_report"
      ],
      "metadata": {
        "id": "htIsnYC88Yjw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "lAPGttEo1DgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw_df(txt_list):\n",
        "  i = 0\n",
        "  all_list = [sentence.strip().split() for sentence in txt_list]\n",
        "  for l in all_list:\n",
        "    if l == []:\n",
        "      i += 1\n",
        "    else:\n",
        "      l.append(f'sentence {i}')\n",
        "  all_list = [x for x in all_list if x and x[4] != \"sentence 0\"]\n",
        "  df = pd.DataFrame(all_list, columns=[\"id\", \"pos_tags\",\"chunk_tags\",\"ner_tags\", \"sentence number\"])\n",
        "  return df\n",
        "\n",
        "def get_preprocessed_df(txt_list, tag_name):\n",
        "  raw_df = get_raw_df(txt_list)\n",
        "  \"\"\"tag_name is a string representing tag name from \"pos_tags\",\"chunk_tags\",\"ner_tags\" \"\"\"\n",
        "  raw_df[\"sentence\"] = raw_df[[\"sentence number\", \"id\", \"ner_tags\"]].groupby([\"sentence number\"])[\"id\"].transform(lambda x: \" \".join(x))\n",
        "  raw_df[\"labels\"] = raw_df[[\"sentence number\", \"id\", \"ner_tags\"]].groupby([\"sentence number\"])[tag_name].transform(lambda x: \" \".join(x))\n",
        "  return raw_df\n"
      ],
      "metadata": {
        "id": "keeKKy3l9HCv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train.txt\") as f:\n",
        "  train = f.readlines()\n",
        "df_train = get_preprocessed_df(train, \"ner_tags\")\n",
        "\n",
        "with open(\"valid.txt\") as f:\n",
        "  valid = f.readlines()\n",
        "df_valid = get_preprocessed_df(valid, \"ner_tags\")\n",
        "\n",
        "with open(\"test.txt\") as f:\n",
        "  test = f.readlines()\n",
        "df_test = get_preprocessed_df(test, \"ner_tags\")"
      ],
      "metadata": {
        "id": "xJC28LD28cWm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of train dataset: {len(df_train)}, Length of validation dataset: {len(df_valid)}, Length of test dataset: {len(df_test)} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ6q7_p1I-vm",
        "outputId": "63f71728-9c47-4aab-8a22-d0865e66b911"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 204566, Length of validation dataset: 51577, Length of test dataset: 46665 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"ner_tags\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oqZGDQJImk0",
        "outputId": "c4d6e640-152b-47a7-b7c4-b3da0dad452c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O         170523\n",
              "B-LOC       7140\n",
              "B-PER       6600\n",
              "B-ORG       6321\n",
              "I-PER       4528\n",
              "I-ORG       3704\n",
              "B-MISC      3438\n",
              "I-LOC       1157\n",
              "I-MISC      1155\n",
              "Name: ner_tags, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_to_ids = {\"O\": 0, \"B-LOC\": 1, \"B-PER\": 2, \"B-ORG\": 3, \"I-PER\": 4, \"I-ORG\": 5, \"B-MISC\": 6, \"I-LOC\": 7, \"I-MISC\": 8}\n",
        "ids_to_labels = dict((v,k) for k,v in labels_to_ids.items())"
      ],
      "metadata": {
        "id": "-3nkfP7VOCoz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_labels(raw_df):\n",
        "  return raw_df[[\"sentence\", \"labels\"]].drop_duplicates().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "TK4zo38mLdEo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = get_sentence_labels(df_train)\n",
        "df_valid = get_sentence_labels(df_valid)\n",
        "df_test = get_sentence_labels(df_test)"
      ],
      "metadata": {
        "id": "seTkKUM_Hn6u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Kb88Z8AnL3O7",
        "outputId": "440ac6c2-6c20-44dc-e3c8-e0115b6183d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b8872a9d-283a-4c89-b230-8642570cdab7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EU rejects German call to boycott British lamb .</td>\n",
              "      <td>B-ORG O B-MISC O O O B-MISC O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Peter Blackburn</td>\n",
              "      <td>B-PER I-PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BRUSSELS 1996-08-22</td>\n",
              "      <td>B-LOC O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The European Commission said on Thursday it di...</td>\n",
              "      <td>O B-ORG I-ORG O O O O O O B-MISC O O O O O B-M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Germany 's representative to the European Unio...</td>\n",
              "      <td>B-LOC O O O O B-ORG I-ORG O O O B-PER I-PER O ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8872a9d-283a-4c89-b230-8642570cdab7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8872a9d-283a-4c89-b230-8642570cdab7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8872a9d-283a-4c89-b230-8642570cdab7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            sentence  \\\n",
              "0   EU rejects German call to boycott British lamb .   \n",
              "1                                    Peter Blackburn   \n",
              "2                                BRUSSELS 1996-08-22   \n",
              "3  The European Commission said on Thursday it di...   \n",
              "4  Germany 's representative to the European Unio...   \n",
              "\n",
              "                                              labels  \n",
              "0                    B-ORG O B-MISC O O O B-MISC O O  \n",
              "1                                        B-PER I-PER  \n",
              "2                                            B-LOC O  \n",
              "3  O B-ORG I-ORG O O O O O O B-MISC O O O O O B-M...  \n",
              "4  B-LOC O O O O B-ORG I-ORG O O O B-PER I-PER O ...  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of train dataset: {len(df_train)}, Length of validation dataset: {len(df_valid)}, Length of test dataset: {len(df_test)} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gE_VmOYPwsN",
        "outputId": "f828c825-13bb-404e-a290-918aa2763362"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 12694, Length of validation dataset: 3072, Length of test dataset: 3188 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change dataframe to PyTorch tensors "
      ],
      "metadata": {
        "id": "hIVKwr0i1ay-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining some hyperparameters for the model.\n",
        "\n",
        "\n",
        "1.   Use Learning rate 0.00001 currently, may tune later.\n",
        "2.   Use tokenizer from bert-base-uncased pretrained model, but this takes words by wordpiece, not that accurate? may improve further\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Nymv_BJ2SmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') #Bert using wordpiece *** may improve further"
      ],
      "metadata": {
        "id": "0mY602GKF5St"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels \n",
        "        sentence = self.data.sentence[index].strip().split()  \n",
        "        word_labels = self.data.labels[index].split() \n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             is_split_into_words=True, #no is_pretokenlized(Modification), we already have a splitted sentence\n",
        "                             return_offsets_mapping=True, \n",
        "                             padding='max_length', \n",
        "                             truncation=True, \n",
        "                             max_length=self.max_len)\n",
        "        \n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in word_labels] \n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "        \n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "          if mapping[0] == 0 and mapping[1] != 0:\n",
        "            # overwrite label\n",
        "            encoded_labels[idx] = labels[i]\n",
        "            i += 1\n",
        "\n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.as_tensor(encoded_labels)\n",
        "        \n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "tEhoO7mcGd4G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = dataset(df_train, tokenizer, MAX_LEN)\n",
        "validation_set = dataset(df_valid, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(df_test, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "axcjPXv1J6P_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GOXUXJlJ7rX",
        "outputId": "ed28094f-15d0-4f03-ef96-9955558e257b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'input_ids': tensor([  101,  7327, 19164,  2446,  2655,  2000, 17757,  2329, 12559,  1012,\n",
              "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'labels': tensor([-100,    3,    0,    6,    0,    0,    0,    6,    0,    0, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100]),\n",
              " 'offset_mapping': tensor([[0, 0],\n",
              "         [0, 2],\n",
              "         [0, 7],\n",
              "         [0, 6],\n",
              "         [0, 4],\n",
              "         [0, 2],\n",
              "         [0, 7],\n",
              "         [0, 7],\n",
              "         [0, 4],\n",
              "         [0, 1],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0]]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "W2hBX04D3Pao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "rLeaK1PYRC6Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRGEqF6fRJ-D",
        "outputId": "39f8b5f3-3bcb-4afb-eaf9-46d34818eb5a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu' #save the processing time\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_3daoOTRb81",
        "outputId": "5eb5f582-5659-4d88-dfbe-221329c5e225"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "Te2SqWJiRkP1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = training_set[0]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"labels\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kjujj9L4m_X",
        "outputId": "a0d2dc51-b694-4b8e-f0a1-78cdd75bb77f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFXj7lRe4rOU",
        "outputId": "ecf3a387-71e7-48f1-9f1a-6f6d5f81bd69"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 7.2604, -0.7276, -0.9241,  ...,  0.2589, -1.6862, -0.5932],\n",
              "         [ 0.0611,  4.1971, -1.1044,  ...,  1.2290, -1.7444, -2.3538],\n",
              "         [ 8.3707, -1.3601, -1.4624,  ..., -0.7933, -1.7581, -0.5170],\n",
              "         ...,\n",
              "         [ 1.5688,  2.1598, -0.6317,  ...,  1.5575, -1.5684, -1.4480],\n",
              "         [ 1.4428,  2.2026, -0.4850,  ...,  1.5535, -1.5579, -1.4704],\n",
              "         [ 1.5983,  1.9302, -0.5046,  ...,  1.6377, -1.5401, -1.3511]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "POrctmH35yfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        \n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "        #loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        tr_loss += output[0]\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        \n",
        "        # only compute accuracy at active labels\n",
        "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "        \n",
        "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_labels.extend(labels)\n",
        "        tr_preds.extend(predictions)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        output[0].backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ],
      "metadata": {
        "id": "H4vL5ObQRr7o"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print(f\"Training epoch: {epoch + 1}\")\n",
        "  train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bezmloKrS8GG",
        "outputId": "28ced118-a6ac-408a-e6c5-939ca8b22a6d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 2.2004306316375732\n",
            "Training loss per 100 training steps: 0.8841642737388611\n",
            "Training loss per 100 training steps: 0.7637917995452881\n",
            "Training loss per 100 training steps: 0.6042304635047913\n",
            "Training loss per 100 training steps: 0.5060256123542786\n",
            "Training loss per 100 training steps: 0.45007655024528503\n",
            "Training loss per 100 training steps: 0.39518532156944275\n",
            "Training loss per 100 training steps: 0.3751581609249115\n",
            "Training loss per 100 training steps: 0.34898972511291504\n",
            "Training loss per 100 training steps: 0.32761529088020325\n",
            "Training loss per 100 training steps: 0.3039153218269348\n",
            "Training loss per 100 training steps: 0.2881028950214386\n",
            "Training loss per 100 training steps: 0.27282842993736267\n",
            "Training loss per 100 training steps: 0.2575330436229706\n",
            "Training loss per 100 training steps: 0.2451707422733307\n",
            "Training loss per 100 training steps: 0.23471219837665558\n",
            "Training loss per 100 training steps: 0.22432264685630798\n",
            "Training loss per 100 training steps: 0.21650755405426025\n",
            "Training loss per 100 training steps: 0.2127857804298401\n",
            "Training loss per 100 training steps: 0.20611180365085602\n",
            "Training loss per 100 training steps: 0.2003677487373352\n",
            "Training loss per 100 training steps: 0.19601042568683624\n",
            "Training loss per 100 training steps: 0.19501446187496185\n",
            "Training loss per 100 training steps: 0.19021065533161163\n",
            "Training loss per 100 training steps: 0.18468952178955078\n",
            "Training loss per 100 training steps: 0.18170273303985596\n",
            "Training loss per 100 training steps: 0.17822390794754028\n",
            "Training loss per 100 training steps: 0.17598935961723328\n",
            "Training loss per 100 training steps: 0.17306382954120636\n",
            "Training loss per 100 training steps: 0.16978490352630615\n",
            "Training loss per 100 training steps: 0.1669619083404541\n",
            "Training loss per 100 training steps: 0.16401606798171997\n",
            "Training loss epoch: 0.16238410770893097\n",
            "Training accuracy epoch: 0.9554698028979528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model"
      ],
      "metadata": {
        "id": "t_DiLpjm53L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            \n",
        "            #loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "\n",
        "            eval_loss += output[0].item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(labels)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ],
      "metadata": {
        "id": "q_vGlAUkuKba"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, predictions = valid(model, testing_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo3ntHnAxA3m",
        "outputId": "bbe56bf8-67b3-4983-8d5b-3454a2e9d3ba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.5878165364265442\n",
            "Validation loss per 100 evaluation steps: 0.08609670852218745\n",
            "Validation loss per 100 evaluation steps: 0.0866909336718031\n",
            "Validation loss per 100 evaluation steps: 0.0951138291464642\n",
            "Validation loss per 100 evaluation steps: 0.08436028769909244\n",
            "Validation loss per 100 evaluation steps: 0.08507894132178957\n",
            "Validation loss per 100 evaluation steps: 0.09463777601672027\n",
            "Validation loss per 100 evaluation steps: 0.09053018444600368\n",
            "Validation loss per 100 evaluation steps: 0.08993140449785231\n",
            "Validation loss per 100 evaluation steps: 0.09399347265655915\n",
            "Validation loss per 100 evaluation steps: 0.0920146119994945\n",
            "Validation loss per 100 evaluation steps: 0.10009727149910601\n",
            "Validation loss per 100 evaluation steps: 0.09936937424129373\n",
            "Validation loss per 100 evaluation steps: 0.10162544719478277\n",
            "Validation loss per 100 evaluation steps: 0.09899976121648604\n",
            "Validation loss per 100 evaluation steps: 0.09559049438813925\n",
            "Validation Loss: 0.09641668835087823\n",
            "Validation Accuracy: 0.9739857346689448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Produce the classification_report"
      ],
      "metadata": {
        "id": "v7IBV4T756bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report([labels], [predictions]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LfBK5TqzbE2",
        "outputId": "9a727306-e7a3-444f-dd28-191743fc6d33"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.86      0.93      0.89      1528\n",
            "        MISC       0.75      0.76      0.76       697\n",
            "         ORG       0.85      0.84      0.84      1611\n",
            "         PER       0.97      0.96      0.97      1614\n",
            "\n",
            "   micro avg       0.87      0.89      0.88      5450\n",
            "   macro avg       0.86      0.87      0.86      5450\n",
            "weighted avg       0.88      0.89      0.88      5450\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model for further use"
      ],
      "metadata": {
        "id": "mMsgNBlk6Vsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UHKor2PU6ZWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}